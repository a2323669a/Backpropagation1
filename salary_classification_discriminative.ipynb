{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nfrom Layer import *\n\ndf \u003d pd.read_csv(\"./data/salary/train_sep.csv\")\ndf_test \u003d pd.read_csv(\"./data/salary/test_sep.csv\")\ntest_x \u003d df_test.values.astype(\u0027float\u0027).T #(dim,n)\n#remember feature scaling\ntest_x \u003d (test_x - np.mean(test_x,axis\u003d1).reshape((-1,1)))/(np.std(test_x,axis\u003d1)+ 1e-10).reshape((-1,1))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": "df_label \u003d df.loc[:, \u0027income\u0027]\ndf_train \u003d df.loc[:, :].drop(\u0027income\u0027, axis\u003d1)\n\ndata_x \u003d df_train.values.astype(np.float).T\ndata_y \u003d df_label.values.astype(np.float).reshape((1, -1))\n\ndata_x \u003d (data_x - np.mean(data_x, axis\u003d1).reshape((-1, 1))) / np.std(data_x, axis\u003d1).reshape((-1, 1))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% prepare data\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "0    ,loss:2895.35251,acc\u003d0.75926\n1    ,loss:2269.40539,acc\u003d0.82852\n2    ,loss:2160.44843,acc\u003d0.83147\n3    ,loss:2094.55878,acc\u003d0.83424\n4    ,loss:2050.81131,acc\u003d0.83581\n5    ,loss:2020.71547,acc\u003d0.83711\n6    ,loss:1994.40363,acc\u003d0.83761\n7    ,loss:1974.79521,acc\u003d0.83773\n8    ,loss:1956.69760,acc\u003d0.83807\n9    ,loss:1941.97882,acc\u003d0.83811",
            "\nsave\n10   ,loss:1931.31626,acc\u003d0.83819\n11   ,loss:1921.58282,acc\u003d0.83854\n12   ,loss:1912.26679,acc\u003d0.83827\n13   ,loss:1903.93237,acc\u003d0.83796\n14   ,loss:1895.58593,acc\u003d0.83788\n15   ,loss:1887.67572,acc\u003d0.83796\n16   ,loss:1883.60721,acc\u003d0.83815\n17   ,loss:1878.75818,acc\u003d0.83777\n18   ,loss:1873.09401,acc\u003d0.83804\n19   ,loss:1869.59426,acc\u003d0.83800\nsave\n20   ,loss:1865.62662,acc\u003d0.83830",
            "\n21   ,loss:1860.39763,acc\u003d0.83819\n22   ,loss:1858.08724,acc\u003d0.83777\n23   ,loss:1853.76997,acc\u003d0.83792\n24   ,loss:1849.57635,acc\u003d0.83796\n25   ,loss:1847.63160,acc\u003d0.83773\n26   ,loss:1845.31582,acc\u003d0.83796\n27   ,loss:1841.88579,acc\u003d0.83784\n28   ,loss:1839.70016,acc\u003d0.83800\n29   ,loss:1837.20725,acc\u003d0.83792\nsave\n30   ,loss:1834.56531,acc\u003d0.83827\n",
            "31   ,loss:1832.28665,acc\u003d0.83796\n32   ,loss:1829.02507,acc\u003d0.83842\n33   ,loss:1827.12596,acc\u003d0.83804\n34   ,loss:1825.80969,acc\u003d0.83800\n35   ,loss:1823.97823,acc\u003d0.83869\n36   ,loss:1821.69186,acc\u003d0.83857\n37   ,loss:1819.96393,acc\u003d0.83842\n38   ,loss:1820.92073,acc\u003d0.83877\n39   ,loss:1817.58175,acc\u003d0.83830\nsave\n40   ,loss:1817.68220,acc\u003d0.83865\n41   ,loss:1815.05894,acc\u003d0.83873\n42   ,loss:1814.24200,acc\u003d0.83880",
            "\n43   ,loss:1812.91086,acc\u003d0.83892\n44   ,loss:1811.98105,acc\u003d0.83896\n45   ,loss:1810.91524,acc\u003d0.83873\n46   ,loss:1809.55456,acc\u003d0.83911\n47   ,loss:1808.24194,acc\u003d0.83900\n48   ,loss:1807.68952,acc\u003d0.83900\n49   ,loss:1806.29667,acc\u003d0.83892",
            "\nsave\n50   ,loss:1805.65524,acc\u003d0.83857\n51   ,loss:1804.64916,acc\u003d0.83857\n52   ,loss:1804.12587,acc\u003d0.83854\n53   ,loss:1802.59417,acc\u003d0.83823\n54   ,loss:1802.51226,acc\u003d0.83888\n55   ,loss:1801.43305,acc\u003d0.83857\n56   ,loss:1800.87030,acc\u003d0.83861",
            "\n57   ,loss:1799.58353,acc\u003d0.83846\n58   ,loss:1799.65758,acc\u003d0.83892\n59   ,loss:1798.13133,acc\u003d0.83857\nsave\n60   ,loss:1797.41652,acc\u003d0.83880\n61   ,loss:1796.78074,acc\u003d0.83861\n",
            "62   ,loss:1796.45374,acc\u003d0.83857\n63   ,loss:1794.70090,acc\u003d0.83819\n64   ,loss:1794.49450,acc\u003d0.83854\n65   ,loss:1793.01799,acc\u003d0.83788\n66   ,loss:1792.79728,acc\u003d0.83815\n67   ,loss:1791.77096,acc\u003d0.83777\n68   ,loss:1791.49454,acc\u003d0.83777",
            "\n69   ,loss:1790.18273,acc\u003d0.83796\nsave\n70   ,loss:1790.02166,acc\u003d0.83811\n71   ,loss:1789.20448,acc\u003d0.83815\n",
            "72   ,loss:1788.41054,acc\u003d0.83769\n73   ,loss:1787.34170,acc\u003d0.83781\n74   ,loss:1787.60363,acc\u003d0.83773\n75   ,loss:1786.16986,acc\u003d0.83754\n76   ,loss:1785.23869,acc\u003d0.83784\n77   ,loss:1784.56740,acc\u003d0.83784\n78   ,loss:1784.06603,acc\u003d0.83796",
            "\n79   ,loss:1782.81843,acc\u003d0.83811\nsave\n80   ,loss:1782.17027,acc\u003d0.83773\n81   ,loss:1781.71162,acc\u003d0.83784\n82   ,loss:1780.80918,acc\u003d0.83796\n83   ,loss:1780.14758,acc\u003d0.83815\n84   ,loss:1779.73916,acc\u003d0.83773",
            "\n85   ,loss:1779.12543,acc\u003d0.83811\n86   ,loss:1778.43055,acc\u003d0.83796\n87   ,loss:1778.15034,acc\u003d0.83773\n88   ,loss:1777.16744,acc\u003d0.83819\n89   ,loss:1777.00258,acc\u003d0.83800\nsave\n90   ,loss:1776.09296,acc\u003d0.83784\n91   ,loss:1776.03586,acc\u003d0.83769\n92   ,loss:1775.62140,acc\u003d0.83788\n93   ,loss:1775.07784,acc\u003d0.83792",
            "\n94   ,loss:1774.53233,acc\u003d0.83819\n95   ,loss:1773.68499,acc\u003d0.83781\n96   ,loss:1773.79739,acc\u003d0.83788\n97   ,loss:1772.68013,acc\u003d0.83784\n98   ,loss:1772.56058,acc\u003d0.83765\n99   ,loss:1771.62996,acc\u003d0.83761\nsave\n100  ,loss:1771.19097,acc\u003d0.83784",
            "\n101  ,loss:1770.66403,acc\u003d0.83758\n102  ,loss:1770.48742,acc\u003d0.83769\n103  ,loss:1769.71083,acc\u003d0.83773\n104  ,loss:1769.41634,acc\u003d0.83796\n105  ,loss:1769.52787,acc\u003d0.83800\n106  ,loss:1768.86222,acc\u003d0.83811\n107  ,loss:1768.18379,acc\u003d0.83819",
            "\n108  ,loss:1768.38103,acc\u003d0.83830\n109  ,loss:1767.78511,acc\u003d0.83827\nsave\n110  ,loss:1767.50795,acc\u003d0.83823\n",
            "111  ,loss:1767.26933,acc\u003d0.83838\n112  ,loss:1767.10004,acc\u003d0.83834\n113  ,loss:1766.55144,acc\u003d0.83861\n114  ,loss:1765.89882,acc\u003d0.83834\n115  ,loss:1765.47876,acc\u003d0.83854\n116  ,loss:1765.13629,acc\u003d0.83869\n117  ,loss:1764.67594,acc\u003d0.83850",
            "\n118  ,loss:1764.19137,acc\u003d0.83857\n119  ,loss:1763.91205,acc\u003d0.83865\nsave\n120  ,loss:1763.56600,acc\u003d0.83877\n",
            "121  ,loss:1762.85746,acc\u003d0.83873\n122  ,loss:1762.14374,acc\u003d0.83842\n123  ,loss:1761.52326,acc\u003d0.83838\n124  ,loss:1761.21275,acc\u003d0.83850\n125  ,loss:1760.67738,acc\u003d0.83846\n126  ,loss:1760.65685,acc\u003d0.83854\n127  ,loss:1760.11951,acc\u003d0.83854\n",
            "128  ,loss:1760.05218,acc\u003d0.83873\n129  ,loss:1759.64936,acc\u003d0.83888\nsave\n130  ,loss:1759.06369,acc\u003d0.83896\n131  ,loss:1759.13549,acc\u003d0.83896\n",
            "132  ,loss:1758.60077,acc\u003d0.83861\n133  ,loss:1757.97983,acc\u003d0.83884\n134  ,loss:1757.85832,acc\u003d0.83884\n135  ,loss:1757.12610,acc\u003d0.83854\n136  ,loss:1757.26029,acc\u003d0.83869\n137  ,loss:1756.81952,acc\u003d0.83884\n138  ,loss:1756.37803,acc\u003d0.83869\n139  ,loss:1756.25198,acc\u003d0.83888\nsave\n140  ,loss:1755.70068,acc\u003d0.83903\n141  ,loss:1755.75873,acc\u003d0.83896\n142  ,loss:1755.90120,acc\u003d0.83900",
            "\n143  ,loss:1755.20557,acc\u003d0.83907\n144  ,loss:1754.60748,acc\u003d0.83946\n145  ,loss:1755.01370,acc\u003d0.83919\n146  ,loss:1753.92492,acc\u003d0.83961\n147  ,loss:1753.83748,acc\u003d0.83949\n148  ,loss:1753.52900,acc\u003d0.83969\n149  ,loss:1753.30603,acc\u003d0.83980\nsave\n",
            "150  ,loss:1753.12547,acc\u003d0.83988\n151  ,loss:1752.85640,acc\u003d0.83969\n152  ,loss:1752.11238,acc\u003d0.83965\n153  ,loss:1751.98526,acc\u003d0.83969\n154  ,loss:1751.64508,acc\u003d0.83988\n155  ,loss:1751.34949,acc\u003d0.83976",
            "\n156  ,loss:1751.18042,acc\u003d0.83992\n157  ,loss:1750.86324,acc\u003d0.83976\n158  ,loss:1751.06929,acc\u003d0.83976\n159  ,loss:1750.63110,acc\u003d0.83984\nsave\n",
            "160  ,loss:1750.24589,acc\u003d0.83984\n161  ,loss:1749.98397,acc\u003d0.83984\n162  ,loss:1750.08269,acc\u003d0.83984\n163  ,loss:1749.45693,acc\u003d0.83999\n164  ,loss:1749.36437,acc\u003d0.83969\n165  ,loss:1750.17312,acc\u003d0.84015\n166  ,loss:1748.85911,acc\u003d0.84003",
            "\n167  ,loss:1748.75645,acc\u003d0.84022\n168  ,loss:1749.12788,acc\u003d0.84015\n169  ,loss:1748.57759,acc\u003d0.84015\nsave\n170  ,loss:1748.44470,acc\u003d0.83984\n171  ,loss:1747.89388,acc\u003d0.84019\n172  ,loss:1747.84919,acc\u003d0.84030",
            "\n173  ,loss:1746.83390,acc\u003d0.84045\n174  ,loss:1747.23040,acc\u003d0.84045\n175  ,loss:1746.38204,acc\u003d0.84049\n176  ,loss:1746.51425,acc\u003d0.84080\n177  ,loss:1746.52088,acc\u003d0.84068\n178  ,loss:1746.12243,acc\u003d0.84057\n179  ,loss:1745.50445,acc\u003d0.84049\nsave\n",
            "180  ,loss:1746.07865,acc\u003d0.84038\n181  ,loss:1745.08857,acc\u003d0.84045\n182  ,loss:1745.19757,acc\u003d0.84034\n183  ,loss:1745.17417,acc\u003d0.84045\n184  ,loss:1744.50482,acc\u003d0.84045\n",
            "185  ,loss:1744.47782,acc\u003d0.84038\n186  ,loss:1744.26734,acc\u003d0.84049\n187  ,loss:1743.70060,acc\u003d0.84042\n188  ,loss:1743.69707,acc\u003d0.84034\n189  ,loss:1743.42047,acc\u003d0.84030\nsave\n190  ,loss:1743.40892,acc\u003d0.84015\n191  ,loss:1742.83039,acc\u003d0.84034\n192  ,loss:1742.53970,acc\u003d0.84038\n193  ,loss:1742.06778,acc\u003d0.84030\n194  ,loss:1742.01813,acc\u003d0.84049\n195  ,loss:1741.40143,acc\u003d0.84015",
            "\n196  ,loss:1741.50966,acc\u003d0.84011\n197  ,loss:1741.41344,acc\u003d0.84030\n198  ,loss:1740.70514,acc\u003d0.84011\n199  ,loss:1741.21031,acc\u003d0.84026\nsave\n200  ,loss:1740.54982,acc\u003d0.84030\n201  ,loss:1740.38571,acc\u003d0.83999\n",
            "202  ,loss:1740.65540,acc\u003d0.84026\n203  ,loss:1739.90462,acc\u003d0.84015\n204  ,loss:1740.00056,acc\u003d0.84022\n205  ,loss:1739.79849,acc\u003d0.84026\n206  ,loss:1739.53953,acc\u003d0.84053\n207  ,loss:1739.11545,acc\u003d0.84019\n208  ,loss:1739.63673,acc\u003d0.84019\n209  ,loss:1739.24874,acc\u003d0.84015\nsave\n210  ,loss:1738.66297,acc\u003d0.84015",
            "\n211  ,loss:1738.89433,acc\u003d0.84015\n212  ,loss:1738.66930,acc\u003d0.84022\n213  ,loss:1738.74480,acc\u003d0.84030\n214  ,loss:1738.45655,acc\u003d0.83980\n",
            "215  ,loss:1738.58174,acc\u003d0.84007\n216  ,loss:1737.90064,acc\u003d0.84034\n217  ,loss:1738.30970,acc\u003d0.83999\n218  ,loss:1738.06987,acc\u003d0.84015\n219  ,loss:1738.10211,acc\u003d0.83992\nsave\n220  ,loss:1738.16136,acc\u003d0.83996\n221  ,loss:1737.92170,acc\u003d0.84026\n222  ,loss:1737.75840,acc\u003d0.84007",
            "\n223  ,loss:1737.55369,acc\u003d0.84011\n224  ,loss:1737.34215,acc\u003d0.83996\n225  ,loss:1737.42692,acc\u003d0.84015\n226  ,loss:1737.13420,acc\u003d0.83980\n227  ,loss:1737.63205,acc\u003d0.83996\n",
            "228  ,loss:1737.08613,acc\u003d0.83976\n229  ,loss:1737.04404,acc\u003d0.84003\nsave\n230  ,loss:1737.28404,acc\u003d0.83980\n231  ,loss:1736.83426,acc\u003d0.83988\n232  ,loss:1736.49457,acc\u003d0.83980\n233  ,loss:1736.90259,acc\u003d0.83988\n234  ,loss:1736.45550,acc\u003d0.83973\n235  ,loss:1736.42423,acc\u003d0.83973\n236  ,loss:1736.69429,acc\u003d0.83942",
            "\n237  ,loss:1736.41797,acc\u003d0.83973\n238  ,loss:1736.20580,acc\u003d0.83996\n239  ,loss:1736.26838,acc\u003d0.83973\nsave\n240  ,loss:1736.36725,acc\u003d0.83984\n241  ,loss:1735.93511,acc\u003d0.83984\n",
            "242  ,loss:1735.94791,acc\u003d0.83988\n243  ,loss:1735.85565,acc\u003d0.84003\n244  ,loss:1735.63767,acc\u003d0.83988\n245  ,loss:1736.08843,acc\u003d0.83976\n246  ,loss:1735.49443,acc\u003d0.83969\n247  ,loss:1735.25729,acc\u003d0.83984\n248  ,loss:1735.71997,acc\u003d0.83969\n249  ,loss:1735.28102,acc\u003d0.83949\nsave\n250  ,loss:1735.14087,acc\u003d0.83953",
            "\n251  ,loss:1735.21636,acc\u003d0.83949\n252  ,loss:1735.16766,acc\u003d0.83973\n253  ,loss:1734.94616,acc\u003d0.83973\n254  ,loss:1734.97629,acc\u003d0.83957\n255  ,loss:1734.73127,acc\u003d0.83949\n256  ,loss:1734.59238,acc\u003d0.83961\n",
            "257  ,loss:1734.44292,acc\u003d0.83949\n258  ,loss:1734.14103,acc\u003d0.83961\n259  ,loss:1734.23404,acc\u003d0.83976\nsave\n260  ,loss:1734.10309,acc\u003d0.83992\n261  ,loss:1733.75392,acc\u003d0.83976\n262  ,loss:1733.91760,acc\u003d0.83961\n263  ,loss:1733.45208,acc\u003d0.83969\n264  ,loss:1733.76270,acc\u003d0.83953\n265  ,loss:1733.40275,acc\u003d0.83980\n266  ,loss:1733.43268,acc\u003d0.83942",
            "\n267  ,loss:1733.08041,acc\u003d0.83984\n268  ,loss:1733.29800,acc\u003d0.83949\n269  ,loss:1733.06297,acc\u003d0.83976\nsave\n270  ,loss:1733.11891,acc\u003d0.83957\n271  ,loss:1732.81403,acc\u003d0.83980\n272  ,loss:1732.93997,acc\u003d0.83957\n273  ,loss:1732.81337,acc\u003d0.83953",
            "\n274  ,loss:1732.55421,acc\u003d0.83949\n275  ,loss:1732.48152,acc\u003d0.83953\n276  ,loss:1732.23765,acc\u003d0.83980\n277  ,loss:1732.74565,acc\u003d0.83946\n278  ,loss:1732.31201,acc\u003d0.83973\n279  ,loss:1732.24113,acc\u003d0.83949",
            "\nsave\n280  ,loss:1732.49658,acc\u003d0.83953\n281  ,loss:1732.05139,acc\u003d0.83942\n282  ,loss:1732.06872,acc\u003d0.83949\n283  ,loss:1732.02482,acc\u003d0.83961\n",
            "284  ,loss:1732.15197,acc\u003d0.83949\n285  ,loss:1731.95414,acc\u003d0.83961\n286  ,loss:1732.05008,acc\u003d0.83976\n287  ,loss:1731.80369,acc\u003d0.83961\n288  ,loss:1731.47017,acc\u003d0.83949\n289  ,loss:1731.89655,acc\u003d0.83953",
            "\nsave\n290  ,loss:1731.82019,acc\u003d0.83965\n291  ,loss:1731.51060,acc\u003d0.83957\n292  ,loss:1731.87366,acc\u003d0.83973\n293  ,loss:1731.56431,acc\u003d0.83973\n294  ,loss:1731.55645,acc\u003d0.83961\n295  ,loss:1731.47942,acc\u003d0.83973\n296  ,loss:1731.32534,acc\u003d0.83988",
            "\n297  ,loss:1731.40340,acc\u003d0.83992\n298  ,loss:1731.44329,acc\u003d0.83988\n299  ,loss:1731.25197,acc\u003d0.83999\nsave\n300  ,loss:1731.16799,acc\u003d0.84003\n",
            "301  ,loss:1731.11995,acc\u003d0.83969\n302  ,loss:1731.18933,acc\u003d0.83988\n303  ,loss:1731.02071,acc\u003d0.83984\n304  ,loss:1731.20157,acc\u003d0.84011\n",
            "305  ,loss:1730.83552,acc\u003d0.83992\n306  ,loss:1730.78237,acc\u003d0.84003\n307  ,loss:1730.61487,acc\u003d0.83999\n308  ,loss:1730.74531,acc\u003d0.83996\n309  ,loss:1730.31814,acc\u003d0.83973\nsave\n",
            "310  ,loss:1730.31792,acc\u003d0.83992\n311  ,loss:1730.23131,acc\u003d0.83988\n312  ,loss:1730.01035,acc\u003d0.83976\n313  ,loss:1729.96385,acc\u003d0.83980\n",
            "314  ,loss:1729.63992,acc\u003d0.83965\n315  ,loss:1729.63502,acc\u003d0.83984\n316  ,loss:1729.56209,acc\u003d0.83957\n317  ,loss:1729.48228,acc\u003d0.83973\n318  ,loss:1729.10663,acc\u003d0.83969",
            "\n319  ,loss:1729.21071,acc\u003d0.83980\nsave\n320  ,loss:1729.05855,acc\u003d0.83942\n321  ,loss:1728.79324,acc\u003d0.83926\n322  ,loss:1729.12639,acc\u003d0.83961\n",
            "323  ,loss:1728.67433,acc\u003d0.83965\n324  ,loss:1728.53796,acc\u003d0.83980\n325  ,loss:1728.48576,acc\u003d0.83969\n326  ,loss:1728.49683,acc\u003d0.83969\n327  ,loss:1728.26719,acc\u003d0.83949\n328  ,loss:1728.35668,acc\u003d0.83957\n329  ,loss:1728.22123,acc\u003d0.83976\nsave\n330  ,loss:1728.25285,acc\u003d0.83969",
            "\n331  ,loss:1728.03191,acc\u003d0.83953\n332  ,loss:1728.10252,acc\u003d0.83980\n333  ,loss:1727.81360,acc\u003d0.83946\n334  ,loss:1728.12047,acc\u003d0.83938\n335  ,loss:1728.01679,acc\u003d0.83965\n",
            "336  ,loss:1727.81632,acc\u003d0.83953\n337  ,loss:1728.49983,acc\u003d0.83965\n338  ,loss:1727.86403,acc\u003d0.83946\n339  ,loss:1727.62028,acc\u003d0.83953\nsave\n340  ,loss:1727.84609,acc\u003d0.83946\n341  ,loss:1727.73112,acc\u003d0.83957\n342  ,loss:1727.71874,acc\u003d0.83957\n343  ,loss:1727.49383,acc\u003d0.83942\n344  ,loss:1727.69154,acc\u003d0.83946",
            "\n345  ,loss:1727.59558,acc\u003d0.83946\n346  ,loss:1727.64577,acc\u003d0.83949\n347  ,loss:1727.18272,acc\u003d0.83949\n348  ,loss:1727.56274,acc\u003d0.83946\n349  ,loss:1727.39089,acc\u003d0.83949",
            "\nsave\n350  ,loss:1727.31683,acc\u003d0.83946\n351  ,loss:1727.23941,acc\u003d0.83942\n352  ,loss:1727.28985,acc\u003d0.83957\n353  ,loss:1727.19655,acc\u003d0.83938\n354  ,loss:1727.42225,acc\u003d0.83938\n355  ,loss:1726.95585,acc\u003d0.83938\n356  ,loss:1727.13763,acc\u003d0.83953\n",
            "357  ,loss:1726.90077,acc\u003d0.83923\n358  ,loss:1727.19559,acc\u003d0.83942\n359  ,loss:1727.06646,acc\u003d0.83953\nsave\n360  ,loss:1726.82821,acc\u003d0.83930\n361  ,loss:1726.64969,acc\u003d0.83938\n362  ,loss:1726.93976,acc\u003d0.83946\n363  ,loss:1726.63549,acc\u003d0.83934",
            "\n364  ,loss:1726.89877,acc\u003d0.83930\n365  ,loss:1726.69233,acc\u003d0.83926\n366  ,loss:1726.70628,acc\u003d0.83938\n367  ,loss:1726.82444,acc\u003d0.83934\n368  ,loss:1726.63968,acc\u003d0.83923\n369  ,loss:1726.46572,acc\u003d0.83926\nsave\n370  ,loss:1726.71743,acc\u003d0.83923",
            "\n371  ,loss:1726.50068,acc\u003d0.83919\n372  ,loss:1726.54178,acc\u003d0.83926\n373  ,loss:1726.54776,acc\u003d0.83919\n374  ,loss:1726.45642,acc\u003d0.83926\n375  ,loss:1726.24585,acc\u003d0.83923\n376  ,loss:1726.53775,acc\u003d0.83900\n",
            "377  ,loss:1726.37688,acc\u003d0.83919\n378  ,loss:1726.55689,acc\u003d0.83919\n379  ,loss:1726.29373,acc\u003d0.83915\nsave\n380  ,loss:1726.27857,acc\u003d0.83930\n381  ,loss:1726.40561,acc\u003d0.83934\n382  ,loss:1726.26516,acc\u003d0.83934\n383  ,loss:1726.31035,acc\u003d0.83934",
            "\n384  ,loss:1726.22666,acc\u003d0.83938\n385  ,loss:1726.04311,acc\u003d0.83946\n386  ,loss:1726.21100,acc\u003d0.83953\n387  ,loss:1726.14629,acc\u003d0.83942\n388  ,loss:1726.23333,acc\u003d0.83949\n389  ,loss:1726.01712,acc\u003d0.83934",
            "\nsave\n390  ,loss:1726.15575,acc\u003d0.83938\n391  ,loss:1725.96489,acc\u003d0.83946\n392  ,loss:1725.88796,acc\u003d0.83949\n393  ,loss:1726.13730,acc\u003d0.83934\n394  ,loss:1725.71712,acc\u003d0.83923\n395  ,loss:1726.10393,acc\u003d0.83942\n396  ,loss:1725.87089,acc\u003d0.83949\n397  ,loss:1726.01699,acc\u003d0.83934\n",
            "398  ,loss:1725.87077,acc\u003d0.83938\n399  ,loss:1725.80061,acc\u003d0.83946\nsave\n400  ,loss:1725.78919,acc\u003d0.83946\n401  ,loss:1725.89757,acc\u003d0.83942\n",
            "402  ,loss:1725.68557,acc\u003d0.83949\n403  ,loss:1725.80949,acc\u003d0.83953\n404  ,loss:1725.77337,acc\u003d0.83938\n405  ,loss:1725.78150,acc\u003d0.83953\n",
            "406  ,loss:1725.59808,acc\u003d0.83942\n407  ,loss:1725.66324,acc\u003d0.83953\n408  ,loss:1725.46619,acc\u003d0.83961\n409  ,loss:1725.48972,acc\u003d0.83976\nsave\n410  ,loss:1725.35471,acc\u003d0.83961\n411  ,loss:1725.61316,acc\u003d0.83965\n412  ,loss:1725.28754,acc\u003d0.83969",
            "\n413  ,loss:1725.40914,acc\u003d0.83969\n414  ,loss:1725.18239,acc\u003d0.83996\n415  ,loss:1725.20899,acc\u003d0.83946\n416  ,loss:1724.99451,acc\u003d0.83965\n417  ,loss:1725.06358,acc\u003d0.83976\n418  ,loss:1724.91890,acc\u003d0.83961\n419  ,loss:1725.18646,acc\u003d0.83965\nsave\n420  ,loss:1724.59807,acc\u003d0.83973",
            "\n421  ,loss:1724.51552,acc\u003d0.83976\n422  ,loss:1724.48527,acc\u003d0.83961\n423  ,loss:1724.29030,acc\u003d0.83973\n424  ,loss:1724.24274,acc\u003d0.83965\n",
            "425  ,loss:1724.21810,acc\u003d0.83969\n426  ,loss:1724.19530,acc\u003d0.83961\n427  ,loss:1723.94605,acc\u003d0.83965\n428  ,loss:1723.96795,acc\u003d0.83957\n429  ,loss:1723.96211,acc\u003d0.83953\nsave\n430  ,loss:1724.22094,acc\u003d0.83961\n431  ,loss:1723.81731,acc\u003d0.83980",
            "\n432  ,loss:1723.66943,acc\u003d0.83973\n433  ,loss:1723.63448,acc\u003d0.83976\n434  ,loss:1723.66642,acc\u003d0.83976\n435  ,loss:1723.59669,acc\u003d0.83984\n436  ,loss:1723.31833,acc\u003d0.83973",
            "\n437  ,loss:1723.28352,acc\u003d0.83984\n438  ,loss:1723.01206,acc\u003d0.83992\n439  ,loss:1723.09932,acc\u003d0.83999\nsave\n440  ,loss:1723.10379,acc\u003d0.83992\n441  ,loss:1722.56860,acc\u003d0.83996\n442  ,loss:1722.82172,acc\u003d0.83999\n443  ,loss:1722.53372,acc\u003d0.83996",
            "\n444  ,loss:1722.56248,acc\u003d0.83999\n445  ,loss:1722.42902,acc\u003d0.84019\n446  ,loss:1722.42312,acc\u003d0.84019\n447  ,loss:1722.36229,acc\u003d0.84011\n448  ,loss:1722.12349,acc\u003d0.84015\n449  ,loss:1722.25684,acc\u003d0.84019\nsave\n450  ,loss:1722.02690,acc\u003d0.84007\n",
            "451  ,loss:1722.02026,acc\u003d0.84030\n452  ,loss:1722.04180,acc\u003d0.84019\n453  ,loss:1721.91155,acc\u003d0.84022\n454  ,loss:1721.83408,acc\u003d0.84022\n455  ,loss:1721.74199,acc\u003d0.84022\n456  ,loss:1721.61821,acc\u003d0.84026\n457  ,loss:1721.91092,acc\u003d0.84038\n458  ,loss:1721.52905,acc\u003d0.84019\n459  ,loss:1721.77064,acc\u003d0.84026\nsave\n460  ,loss:1721.49119,acc\u003d0.84034",
            "\n461  ,loss:1721.50268,acc\u003d0.84019\n462  ,loss:1721.52281,acc\u003d0.84011\n463  ,loss:1721.52823,acc\u003d0.84038\n464  ,loss:1721.53936,acc\u003d0.84011\n465  ,loss:1721.30567,acc\u003d0.84022\n",
            "466  ,loss:1721.51809,acc\u003d0.84011\n467  ,loss:1721.26176,acc\u003d0.84019\n468  ,loss:1721.49539,acc\u003d0.83980\n469  ,loss:1721.43066,acc\u003d0.84003\nsave\n470  ,loss:1721.31092,acc\u003d0.84030\n471  ,loss:1721.31888,acc\u003d0.83988\n472  ,loss:1721.35033,acc\u003d0.84011",
            "\n473  ,loss:1721.37807,acc\u003d0.84011\n474  ,loss:1721.24212,acc\u003d0.83988\n475  ,loss:1721.25427,acc\u003d0.84034\n476  ,loss:1721.18603,acc\u003d0.84019\n",
            "477  ,loss:1721.28100,acc\u003d0.84019\n478  ,loss:1721.27426,acc\u003d0.83999\n479  ,loss:1720.98688,acc\u003d0.84011\nsave\n480  ,loss:1721.25006,acc\u003d0.84019\n481  ,loss:1721.15131,acc\u003d0.84007\n482  ,loss:1720.98154,acc\u003d0.84026\n483  ,loss:1721.03014,acc\u003d0.84019\n484  ,loss:1721.09122,acc\u003d0.84030\n485  ,loss:1721.08330,acc\u003d0.84015\n486  ,loss:1720.95658,acc\u003d0.84011",
            "\n487  ,loss:1721.01411,acc\u003d0.84003\n488  ,loss:1720.99939,acc\u003d0.84011\n489  ,loss:1720.97216,acc\u003d0.84003\nsave\n490  ,loss:1720.84558,acc\u003d0.84019\n491  ,loss:1720.99017,acc\u003d0.83996\n492  ,loss:1720.79316,acc\u003d0.84003\n493  ,loss:1720.89266,acc\u003d0.84003",
            "\n494  ,loss:1720.98148,acc\u003d0.83992\n495  ,loss:1720.85830,acc\u003d0.84019\n496  ,loss:1720.76247,acc\u003d0.83999\n497  ,loss:1720.85332,acc\u003d0.83992\n498  ,loss:1720.88486,acc\u003d0.83976\n499  ,loss:1720.63188,acc\u003d0.83992\nsave\n500  ,loss:1720.82917,acc\u003d0.84003\n501  ,loss:1720.71875,acc\u003d0.83980\n502  ,loss:1721.19623,acc\u003d0.84011\n503  ,loss:1720.75124,acc\u003d0.83999",
            "\n504  ,loss:1720.68609,acc\u003d0.84007\n505  ,loss:1720.69667,acc\u003d0.83988\n506  ,loss:1720.61511,acc\u003d0.84003\n507  ,loss:1720.74422,acc\u003d0.83984\n508  ,loss:1720.48211,acc\u003d0.84007\n509  ,loss:1720.57178,acc\u003d0.83996\nsave\n",
            "510  ,loss:1720.60130,acc\u003d0.83992\n511  ,loss:1720.61297,acc\u003d0.83984\n512  ,loss:1720.58781,acc\u003d0.83973\n513  ,loss:1720.52967,acc\u003d0.83988\n514  ,loss:1720.52472,acc\u003d0.83988\n515  ,loss:1720.54802,acc\u003d0.83984\n516  ,loss:1720.54259,acc\u003d0.83980\n517  ,loss:1720.52834,acc\u003d0.84003",
            "\n518  ,loss:1720.43520,acc\u003d0.84015\n519  ,loss:1720.46711,acc\u003d0.83996\nsave\n520  ,loss:1720.47947,acc\u003d0.83984\n521  ,loss:1720.34791,acc\u003d0.83999\n522  ,loss:1720.47207,acc\u003d0.84011\n523  ,loss:1720.48691,acc\u003d0.84003",
            "\n524  ,loss:1720.39977,acc\u003d0.84003\n525  ,loss:1720.35840,acc\u003d0.83988\n526  ,loss:1720.35223,acc\u003d0.83996\n527  ,loss:1720.47455,acc\u003d0.83999\n528  ,loss:1720.29396,acc\u003d0.83996\n529  ,loss:1720.31906,acc\u003d0.84026\nsave\n530  ,loss:1720.24989,acc\u003d0.83988",
            "\n531  ,loss:1720.37333,acc\u003d0.84019\n532  ,loss:1720.21010,acc\u003d0.83999\n533  ,loss:1720.25272,acc\u003d0.84011\n534  ,loss:1720.24079,acc\u003d0.83999\n",
            "535  ,loss:1720.25524,acc\u003d0.83999\n536  ,loss:1720.36510,acc\u003d0.84011\n537  ,loss:1720.33875,acc\u003d0.83996\n538  ,loss:1720.11405,acc\u003d0.84007\n539  ,loss:1720.12424,acc\u003d0.83992\nsave\n540  ,loss:1720.15689,acc\u003d0.84007\n"
          ],
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-19-3107189c5179\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 73\u001b[1;33m             \u001b[0md_c_a\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_c_a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[1;33m+\u003d\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\PycharmProjects\\Backpropagation1\\Layer.py\u001b[0m in \u001b[0;36mbackpropagation\u001b[1;34m(self, d_c_a, lr)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#(dim,neurons) @ [(neurons,n) * (neurons,n)] -\u003e (dim,n)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 32\u001b[1;33m         \u001b[0md_c_a0\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_rev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_c_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#update parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\PycharmProjects\\Backpropagation1\\Layer.py\u001b[0m in \u001b[0;36msigmoid_rev\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid_rev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 51\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\PycharmProjects\\Backpropagation1\\Layer.py\u001b[0m in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 47\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "source": "def loss(pred, y):\n    pred_log \u003d np.log(pred + 1e-20)  # q(1) \u003d f(x)\n    no_pred_log \u003d np.log(1.0 - pred + 1e-20)  # q(0) \u003d 1 - f(x) ; f(x) : probability of class A(1)\n    loss_val \u003d -1. * (np.matmul(y, pred_log.T) + np.matmul((1.0 - y), no_pred_log.T))\n    return loss_val\n\ndef accuarcy(pred,y):\n    pred \u003d np.rint(pred)\n    return np.mean(np.equal(pred,y).astype(\u0027float\u0027))\n\ndef grad_cross_entropy(pred,y):\n    d_c_pred \u003d -1.0 * (y / (pred + 1e-20) - (1. - y) / (1. - pred + 1e-20))  # (1,n) it is grad not loss\n    return d_c_pred\n\ndef shuffle():\n    seed \u003d np.random.randint(0,1e5)\n    \n    np.random.seed(seed)\n    #I make a big error : shuffle\n    #np shuffle is shuffled by the first dim.in this example,is row\n    #but we make the data_x [dim,n].so, we need transpose the data_x first and shuffle next\n    np.random.shuffle(data_x.T)\n    np.random.seed(seed)\n    np.random.shuffle(data_y.T)\n\ndim \u003d data_x.shape[0]\ndim1 \u003d 64\ndim2 \u003d 16\ndim3 \u003d 4\ndim4 \u003d 2\ndim5 \u003d 1\n\nlayers \u003d [Layer(dim,dim3,relu,relu_rev),\n          #Layer(dim2,dim4,relu,relu_rev),\n          #Layer(dim2,dim3,relu,relu_rev),\n          #Layer(dim3,dim4,relu,relu_rev),\n          Layer(dim3,dim5,sigmoid,sigmoid_rev)]\nn \u003d np.floor(data_x.shape[1] * 0.2).astype(\"int\")\nlr \u003d 0.3\nepochs \u003d 2000\nbatch_size \u003d 512\nbatch_count \u003d n // batch_size\nloss_list \u003d []\n\ndef predict(epoch):\n    pred \u003d test_x\n    for layer in layers:\n        pred \u003d layer.forward(pred)\n    \n    pred \u003d np.rint(pred).reshape((-1,))#\u003c\u003d50K : 0; \u003e50K : 1\n    pred_list \u003d list(pred)\n    id_list \u003d range(1,len(pred)+1)\n    df_out \u003d pd.DataFrame({\u0027id\u0027:id_list,\u0027label\u0027:pred_list},dtype\u003d\u0027int\u0027)\n    df_out.to_csv(\"./result/salary/pred_{}.csv\".format(epoch),index\u003dFalse)\n\nshuffle()\nfor epoch in range(epochs):\n    loss_val \u003d 0\n    \n    for i in range(batch_count):\n        a \u003d data_x[:,i*batch_size:(i+1)*batch_size]\n        y \u003d data_y[:,i*batch_size:(i+1)*batch_size]\n    \n        # forward\n        for layer in layers:\n            a \u003d layer.forward(a)\n    \n        # cross entropy\n        d_c_a \u003d grad_cross_entropy(a,y)  # (1,n) it is grad not loss\n    \n        # backpropagation\n        for layer in reversed(layers):\n            d_c_a \u003d layer.backpropagation(d_c_a,lr)\n        \n        loss_val +\u003d loss(a, y)[0][0]\n    \n    loss_list.append(loss_val)\n    a \u003d data_x[:,n:]\n    y \u003d data_y[:,n:]\n    for layer in layers:\n            a \u003d layer.forward(a)\n    acc \u003d accuarcy(a,y)\n    if epoch %1 \u003d\u003d 0:\n            print(\"{:\u003c5d},loss:{:.5f},acc\u003d{:.5f}\".format(epoch,loss_val,acc))\n    if (epoch+1) % 10 \u003d\u003d 0:\n        #predict(epoch)\n        print(\"save\")",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\nplt.plot(loss_list)\nplt.show()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}