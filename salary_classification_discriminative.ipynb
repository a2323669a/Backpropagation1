{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nfrom Layer import *\n\ndf \u003d pd.read_csv(\"./data/salary/train_sep.csv\")\ndf_test \u003d pd.read_csv(\"./data/salary/test_sep.csv\")\ntest_x \u003d df_test.values.astype(\u0027float\u0027).T #(dim,n)\n#remember feature scaling\ntest_x \u003d (test_x - np.mean(test_x,axis\u003d1).reshape((-1,1)))/(np.std(test_x,axis\u003d1)+ 1e-10).reshape((-1,1))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "source": "df_label \u003d df.loc[:, \u0027income\u0027]\ndf_train \u003d df.loc[:, :].drop(\u0027income\u0027, axis\u003d1)\n\ndata_x \u003d df_train.values.astype(np.float).T\ndata_y \u003d df_label.values.astype(np.float).reshape((1, -1))\n\ndata_x \u003d (data_x - np.mean(data_x, axis\u003d1).reshape((-1, 1))) / np.std(data_x, axis\u003d1).reshape((-1, 1))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% prepare data\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "0    ,loss:7277.73500,acc\u003d0.84468\n",
            "1    ,loss:6520.49855,acc\u003d0.84875\n",
            "2    ,loss:6348.03117,acc\u003d0.85528\n",
            "3    ,loss:6276.55044,acc\u003d0.86165\n",
            "4    ,loss:6115.43732,acc\u003d0.85850\n",
            "5    ,loss:6080.35525,acc\u003d0.86104\n",
            "6    ,loss:5953.60597,acc\u003d0.85935\n",
            "7    ,loss:5911.66459,acc\u003d0.86035\n",
            "8    ,loss:5893.94511,acc\u003d0.86173\n",
            "9    ,loss:5783.09211,acc\u003d0.86127\nsave\n",
            "10   ,loss:5781.51622,acc\u003d0.86173\n",
            "11   ,loss:5742.28784,acc\u003d0.86311\n",
            "12   ,loss:5756.48239,acc\u003d0.86879\n",
            "13   ,loss:5641.22195,acc\u003d0.86434\n",
            "14   ,loss:5668.62359,acc\u003d0.86779\n",
            "15   ,loss:5632.05452,acc\u003d0.86848\n",
            "16   ,loss:5631.71582,acc\u003d0.86756\n",
            "17   ,loss:5617.73821,acc\u003d0.86702\n",
            "18   ,loss:5610.54802,acc\u003d0.87086\n",
            "19   ,loss:5539.29624,acc\u003d0.86956\nsave\n",
            "20   ,loss:5575.69948,acc\u003d0.87086\n",
            "21   ,loss:5461.13454,acc\u003d0.86779\n",
            "22   ,loss:5463.89735,acc\u003d0.86917\n",
            "23   ,loss:5545.76629,acc\u003d0.87539\n",
            "24   ,loss:5436.19728,acc\u003d0.86956\n",
            "25   ,loss:5510.44607,acc\u003d0.87248\n",
            "26   ,loss:5413.86711,acc\u003d0.87094\n",
            "27   ,loss:5444.78139,acc\u003d0.87163\n",
            "28   ,loss:5442.62345,acc\u003d0.87547\n",
            "29   ,loss:5357.40933,acc\u003d0.87225\nsave\n",
            "30   ,loss:5327.58069,acc\u003d0.87109\n",
            "31   ,loss:5312.47892,acc\u003d0.87317\n",
            "32   ,loss:5276.06961,acc\u003d0.87217\n",
            "33   ,loss:5262.22491,acc\u003d0.87455\n",
            "34   ,loss:5286.55062,acc\u003d0.87493\n",
            "35   ,loss:5236.20261,acc\u003d0.87132\n",
            "36   ,loss:5188.88201,acc\u003d0.87202\n",
            "37   ,loss:5215.64391,acc\u003d0.87194\n",
            "38   ,loss:5317.33194,acc\u003d0.87992\n",
            "39   ,loss:5168.54788,acc\u003d0.87109\nsave\n",
            "40   ,loss:5202.49961,acc\u003d0.87701\n",
            "41   ,loss:5191.45272,acc\u003d0.87447\n",
            "42   ,loss:5199.50604,acc\u003d0.87509\n",
            "43   ,loss:5194.73865,acc\u003d0.87747\n",
            "44   ,loss:5184.29190,acc\u003d0.87893\n",
            "45   ,loss:5272.09156,acc\u003d0.88269\n",
            "46   ,loss:5175.00619,acc\u003d0.88054\n",
            "47   ,loss:5029.43946,acc\u003d0.87547\n",
            "48   ,loss:5143.07481,acc\u003d0.87985\n",
            "49   ,loss:5041.89856,acc\u003d0.87232\nsave\n",
            "50   ,loss:5083.64200,acc\u003d0.87724\n",
            "51   ,loss:5145.58801,acc\u003d0.88154\n",
            "52   ,loss:5047.25958,acc\u003d0.88008\n",
            "53   ,loss:5121.67437,acc\u003d0.87770\n",
            "54   ,loss:5081.90585,acc\u003d0.87877\n",
            "55   ,loss:5119.22990,acc\u003d0.88399\n",
            "56   ,loss:4973.86604,acc\u003d0.87893\n",
            "57   ,loss:5001.45234,acc\u003d0.88008\n",
            "58   ,loss:5065.17215,acc\u003d0.88384\n",
            "59   ,loss:4988.65377,acc\u003d0.88023\n",
            "save\n",
            "60   ,loss:5006.80186,acc\u003d0.87908\n",
            "61   ,loss:5028.94859,acc\u003d0.88322\n",
            "62   ,loss:4928.25730,acc\u003d0.87762\n",
            "63   ,loss:4883.72106,acc\u003d0.87678\n",
            "64   ,loss:4895.59253,acc\u003d0.87977\n",
            "65   ,loss:4908.90873,acc\u003d0.87969\n",
            "66   ,loss:4938.63622,acc\u003d0.88161\n",
            "67   ,loss:4898.44755,acc\u003d0.88031\n",
            "68   ,loss:4913.66979,acc\u003d0.88138\n",
            "69   ,loss:4962.72636,acc\u003d0.88415\nsave\n",
            "70   ,loss:4961.29558,acc\u003d0.88276\n",
            "71   ,loss:4947.42713,acc\u003d0.88100\n",
            "72   ,loss:4890.34025,acc\u003d0.88261\n",
            "73   ,loss:4846.43727,acc\u003d0.87770\n",
            "74   ,loss:4852.43365,acc\u003d0.88445\n",
            "75   ,loss:4914.97605,acc\u003d0.88522\n",
            "76   ,loss:4868.59448,acc\u003d0.88399\n",
            "77   ,loss:4910.64911,acc\u003d0.88407\n",
            "78   ,loss:4970.25161,acc\u003d0.88637\n",
            "79   ,loss:4854.88332,acc\u003d0.88307\nsave\n",
            "80   ,loss:4860.33994,acc\u003d0.88269\n",
            "81   ,loss:4817.95126,acc\u003d0.88215\n",
            "82   ,loss:4841.63301,acc\u003d0.88507\n",
            "83   ,loss:4842.09002,acc\u003d0.88691\n",
            "84   ,loss:4825.63352,acc\u003d0.88230\n",
            "85   ,loss:4804.89681,acc\u003d0.88384\n",
            "86   ,loss:4775.98298,acc\u003d0.88345\n",
            "87   ,loss:4806.28453,acc\u003d0.88422\n",
            "88   ,loss:4844.63229,acc\u003d0.88599\n",
            "89   ,loss:4793.47367,acc\u003d0.88653\nsave\n",
            "90   ,loss:4704.56025,acc\u003d0.88177\n",
            "91   ,loss:4780.56031,acc\u003d0.88399\n",
            "92   ,loss:4704.47270,acc\u003d0.88061\n",
            "93   ,loss:4827.62209,acc\u003d0.88967\n",
            "94   ,loss:4758.05334,acc\u003d0.88345\n",
            "95   ,loss:4753.07196,acc\u003d0.89044\n",
            "96   ,loss:4727.15660,acc\u003d0.88322\n",
            "97   ,loss:4685.83743,acc\u003d0.88215\n",
            "98   ,loss:4722.57271,acc\u003d0.88468\n",
            "99   ,loss:4726.08264,acc\u003d0.88599\nsave\n",
            "100  ,loss:4728.00154,acc\u003d0.88499\n",
            "101  ,loss:4730.91654,acc\u003d0.88752\n",
            "102  ,loss:4661.34145,acc\u003d0.88461\n",
            "103  ,loss:4677.82669,acc\u003d0.88453\n",
            "104  ,loss:4676.05325,acc\u003d0.88599\n",
            "105  ,loss:4648.78081,acc\u003d0.88691\n",
            "106  ,loss:4711.26666,acc\u003d0.88676\n",
            "107  ,loss:4801.04002,acc\u003d0.89067\n",
            "108  ,loss:4725.22472,acc\u003d0.88660\n",
            "109  ,loss:4591.75786,acc\u003d0.88100\nsave\n",
            "110  ,loss:4666.74109,acc\u003d0.88699\n",
            "111  ,loss:4729.14767,acc\u003d0.88775\n",
            "112  ,loss:4621.11110,acc\u003d0.88407\n",
            "113  ,loss:4679.90047,acc\u003d0.88868\n",
            "114  ,loss:4716.75744,acc\u003d0.88952\n",
            "115  ,loss:4673.71860,acc\u003d0.88906\n",
            "116  ,loss:4601.42789,acc\u003d0.88461\n",
            "117  ,loss:4593.50492,acc\u003d0.88622\n",
            "118  ,loss:4580.98201,acc\u003d0.88560\n",
            "119  ,loss:4630.86747,acc\u003d0.88891\nsave\n",
            "120  ,loss:4648.22657,acc\u003d0.89159\n",
            "121  ,loss:4667.70984,acc\u003d0.88591\n",
            "122  ,loss:4685.96878,acc\u003d0.89205\n",
            "123  ,loss:4591.16277,acc\u003d0.88668\n",
            "124  ,loss:4628.81291,acc\u003d0.88768\n",
            "125  ,loss:4654.60834,acc\u003d0.89083\n",
            "126  ,loss:4569.42056,acc\u003d0.88729\n",
            "127  ,loss:4615.86418,acc\u003d0.88814\n",
            "128  ,loss:4527.21362,acc\u003d0.88760\n",
            "129  ,loss:4634.77595,acc\u003d0.88845\nsave\n",
            "130  ,loss:4648.42492,acc\u003d0.88791\n",
            "131  ,loss:4567.31896,acc\u003d0.88990\n",
            "132  ,loss:4509.77359,acc\u003d0.88668\n",
            "133  ,loss:4664.24151,acc\u003d0.89520\n",
            "134  ,loss:4593.60561,acc\u003d0.88837\n",
            "135  ,loss:4586.53267,acc\u003d0.89144\n",
            "136  ,loss:4620.67923,acc\u003d0.88868\n",
            "137  ,loss:4602.28998,acc\u003d0.89098\n",
            "138  ,loss:4562.98789,acc\u003d0.89036\n",
            "139  ,loss:4525.98642,acc\u003d0.89021\nsave\n",
            "140  ,loss:4581.12992,acc\u003d0.88875\n",
            "141  ,loss:4529.38145,acc\u003d0.88821\n",
            "142  ,loss:4574.62086,acc\u003d0.89367\n",
            "143  ,loss:4494.09959,acc\u003d0.89013\n"
          ],
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-23-5cf46e743241\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mloss_val\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 58\u001b[1;33m     \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m\u003cipython-input-23-5cf46e743241\u003e\u001b[0m in \u001b[0;36mshuffle\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#np shuffle is shuffled by the first dim.in this example,is row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#but we make the data_x [dim,n].so, we need transpose the data_x first and shuffle next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 22\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "source": "def loss(pred, y):\n    pred_log \u003d np.log(pred + 1e-20)  # q(1) \u003d f(x)\n    no_pred_log \u003d np.log(1.0 - pred + 1e-20)  # q(0) \u003d 1 - f(x) ; f(x) : probability of class A(1)\n    loss_val \u003d -1. * (np.matmul(y, pred_log.T) + np.matmul((1.0 - y), no_pred_log.T))\n    return loss_val\n\ndef accuarcy(pred,y):\n    pred \u003d np.rint(pred)\n    return np.mean(np.equal(pred,y).astype(\u0027float\u0027))\n\ndef grad_cross_entropy(pred,y):\n    d_c_pred \u003d -1.0 * (y / (pred + 1e-20) - (1. - y) / (1. - pred + 1e-20))  # (1,n) it is grad not loss\n    return d_c_pred\n\ndef shuffle():\n    seed \u003d np.random.randint(0,1e5)\n    \n    np.random.seed(seed)\n    #I make a big error : shuffle\n    #np shuffle is shuffled by the first dim.in this example,is row\n    #but we make the data_x [dim,n].so, we need transpose the data_x first and shuffle next\n    np.random.shuffle(data_x.T)\n    np.random.seed(seed)\n    np.random.shuffle(data_y.T)\n\ndim \u003d data_x.shape[0]\ndim1 \u003d 64\ndim2 \u003d 16\ndim3 \u003d 4\ndim4 \u003d 2\ndim5 \u003d 1\n\nlayers \u003d [Layer(dim,dim1,relu,relu_rev),\n          Layer(dim1,dim3,relu,relu_rev),\n          #Layer(dim2,dim3,relu,relu_rev),\n          #Layer(dim3,dim4,relu,relu_rev),\n          Layer(dim3,dim5,sigmoid,sigmoid_rev)]\nn \u003d np.floor(data_x.shape[1] * 0.6).astype(\"int\")\nlr \u003d 0.1\nepochs \u003d 2000\nbatch_size \u003d 256\nbatch_count \u003d n // batch_size\nloss_list \u003d []\n\ndef predict(epoch):\n    pred \u003d test_x\n    for layer in layers:\n        pred \u003d layer.forward(pred)\n    \n    pred \u003d np.rint(pred).reshape((-1,))#\u003c\u003d50K : 0; \u003e50K : 1\n    pred_list \u003d list(pred)\n    id_list \u003d range(1,len(pred)+1)\n    df_out \u003d pd.DataFrame({\u0027id\u0027:id_list,\u0027label\u0027:pred_list},dtype\u003d\u0027int\u0027)\n    df_out.to_csv(\"./result/salary/pred_{}.csv\".format(epoch),index\u003dFalse)\n\nfor epoch in range(epochs):\n    loss_val \u003d 0\n    shuffle()\n    \n    for i in range(batch_count):\n        a \u003d data_x[:,i*batch_size:(i+1)*batch_size]\n        y \u003d data_y[:,i*batch_size:(i+1)*batch_size]\n    \n        # forward\n        for layer in layers:\n            a \u003d layer.forward(a)\n    \n        # cross entropy\n        d_c_a \u003d grad_cross_entropy(a,y)  # (1,n) it is grad not loss\n    \n        # backpropagation\n        for layer in reversed(layers):\n            d_c_a \u003d layer.backpropagation(d_c_a,lr)\n        \n        loss_val +\u003d loss(a, y)[0][0]\n    \n    loss_list.append(loss_val)\n    a \u003d data_x[:,n:]\n    y \u003d data_y[:,n:]\n    for layer in layers:\n            a \u003d layer.forward(a)\n    acc \u003d accuarcy(a,y)\n    if epoch %1 \u003d\u003d 0:\n            print(\"{:\u003c5d},loss:{:.5f},acc\u003d{:.5f}\".format(epoch,loss_val,acc))\n    if (epoch+1) % 10 \u003d\u003d 0:\n        predict(epoch)\n        print(\"save\")",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\nplt.plot(loss_list)\nplt.show()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}